{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, arch, inputs, outputs, learning_rate):\n",
    "        self.arch = arch\n",
    "        self.input_pairs = [(x, y) for x, y in zip(inputs, outputs)]\n",
    "        self.weights = self.__create_weights(arch)\n",
    "        self.biases = self.__create_biases(arch)\n",
    "        self.error = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activations = None # All neuron outputs from the network\n",
    "        self.output = None\n",
    "        self.training_set = None\n",
    "        self.testing_set = None\n",
    "        self.accuracy = None\n",
    "        \n",
    "    def __sigmoid(self, x):\n",
    "        \"\"\"This is the activation function for the network, it will\n",
    "            squishify the results of the network to be between -1 and 1.\n",
    "            It is non-linear  like all other activation functions\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_prime(self, x):\n",
    "        \"\"\"This is the \"derivative\" of the sigmoid function.\n",
    "            it will be used in the learning phase of the network.\n",
    "            The real derivative of the sigmoid function is\n",
    "            f'(x) = sigmoid(x) x (1- sigmoid(x)). However\n",
    "            when using this in the program my values have already\n",
    "            gone through the sigmoid function so doing so again would be\n",
    "            incorrect\"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    \n",
    "    def __create_weight_layer(self,row, col):\n",
    "        \"\"\"This will create a matrix with random values of the shape\n",
    "                R x C where R = the number of neurons in the next layer\n",
    "                and C = the number of neurons in the current layer\"\"\"\n",
    "\n",
    "        return np.random.randn(row, col)\n",
    "\n",
    "    \n",
    "    def __create_weights(self,arch):\n",
    "        \"\"\"This function will create the weight matrix for an entire network\n",
    "                it takes a list where each member in the list represents the the number\n",
    "                of neurons in that layer. So a input of [5,2,1] represents a network\n",
    "                with 5 neurons in the first layer, 2 in the second and one in the last layer.\n",
    "                It will return a list of n-dimensional numpy arrays where each entry in the list\n",
    "                represents the weights from one layer to the next\"\"\"\n",
    "\n",
    "        # Okay so what's this slicing about? For the architecture of a network we are able to\n",
    "        # deduce the overall structure of the weight matrix. We will have n-1 weight layers\n",
    "        # per network where n is the total number of network layers. There are no weights for\n",
    "        # incoming data so a network shaped like [5,2,1] will have 2 weight layers.\n",
    "        # The first weight layer will be an array with two rows and 5 columns, the second\n",
    "        # 1 row with 2 columns\n",
    "        column_values = arch[:-1]\n",
    "        row_values = arch[1:]\n",
    "\n",
    "        return [self.__create_weight_layer(row, col) for row, col in zip(row_values, column_values)]\n",
    "\n",
    "    \n",
    "    def __create_bias_layer(self,layer):\n",
    "        \"\"\"Creates a np matrix of (n x 1) for a layer of a network\n",
    "                returns the matrix \"\"\"\n",
    "        return np.random.randn(layer, 1)\n",
    "\n",
    "    \n",
    "    def __create_biases(self, arch):\n",
    "        \"\"\"The number of bias matrices for a network is n - 1, where\n",
    "                n is the number of layers in the network, including inputs.\n",
    "                Each neuron in a layer will have it's own bias. So a network\n",
    "                of size [5,2,1] will have two biases for one layer and 1 for the next.\n",
    "                The inputs do not get biases.\"\"\"\n",
    "        layers_that_need_biases = arch[1:]\n",
    "        return [self.__create_bias_layer(neurons_in_layer) for neurons_in_layer in layers_that_need_biases]\n",
    "\n",
    "    def __feed_forward(self, input_matrix_as_list, guess=False):\n",
    "        \"\"\"This function feeds the inputs into the matrix.\"\"\"\n",
    "        # Store inputs as a column matrix (n x 1)\n",
    "        if isinstance(input_matrix_as_list, (np.ndarray, np.generic)):\n",
    "            input_matrix_as_list = input_matrix_as_list.tolist()\n",
    "\n",
    "        output = np.c_[input_matrix_as_list]\n",
    "        layer_activations = [output]\n",
    "\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            output = self.__sigmoid(np.dot(weight, output) + bias)\n",
    "            layer_activations.append(output)\n",
    "\n",
    "        if not guess:\n",
    "            self.output = output\n",
    "            self.activations = layer_activations\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __back_propagate_error(self, actual_values_matrix, guessed_values_matrix):\n",
    "        \"\"\"This function will calculate a error matrix for each hidden layer neuron \"\"\"\n",
    "        actual_values_matrix = np.c_[actual_values_matrix]\n",
    "        guessed_values_matrix = np.c_[guessed_values_matrix]\n",
    "\n",
    "        # seed error with error between output of network and target output\n",
    "        error = [np.subtract(actual_values_matrix, guessed_values_matrix)]\n",
    "\n",
    "        # Reverse the weight matrix and then iterate in reverse to\n",
    "        # the last layer, we omit the first layer as that would calculate\n",
    "        # the error for inputs which does not make sense\n",
    "        for index, layer in enumerate(self.weights[:0:-1]):\n",
    "            # The error for a layer is the dot product of the transposed weight matrix \n",
    "            # and the error of the previous layer\n",
    "            error.append(np.dot(layer.transpose(), error[index]))\n",
    "        self.error = list(reversed(error))\n",
    "\n",
    "    def __create_deltas(self):\n",
    "        \"\"\"This method will find the changes to each of the weights and biases in the network\n",
    "            it will then return these two lists of deltas\"\"\"\n",
    "\n",
    "        # lambda function to apply the derivative of sigmoid to the activations array\n",
    "        def vectorized_sigmoid_prime(x): return self.__sigmoid_prime(x)\n",
    "\n",
    "        # lists to contain the delta for each layer's weights and biases\n",
    "        delta_weights = []\n",
    "        delta_biases = []\n",
    "\n",
    "        # the gradients found from applying the sigmoid prime to the activations, omitting the first layer\n",
    "        # which is the inputs, these cannot be altered\n",
    "        gradients = [vectorized_sigmoid_prime(layer) for layer in self.activations[1:]]\n",
    "\n",
    "        # iterate over the errors, gradients, weights, biases, and activations\n",
    "        # applying the function delta_weight_layer = Gradient * lr * Error Layer * activation.T\n",
    "        # the delta_biases = gradients for that layer\n",
    "        for _, error_layer, gradient, activation, bias in zip(self.weights, self.error, gradients, self.activations,\n",
    "                                                              self.biases):\n",
    "            gradient = np.multiply(gradient, self.learning_rate)\n",
    "            gradient = np.multiply(gradient, error_layer)\n",
    "            delta_layer = np.multiply(activation.transpose(), gradient)\n",
    "            delta_weights.append(delta_layer)\n",
    "            delta_biases.append(gradient)\n",
    "\n",
    "        # return lists of gathered errors\n",
    "        return delta_weights, delta_biases\n",
    "\n",
    "    def train_SGD(self, iterations, train_set_size):\n",
    "        \"\"\"This method will uses stochastic gradient descent to train the network. It will\n",
    "            select a random entry in the training set, which is tuple of lists where [0]: inputs\n",
    "            and where [1]: 1 is the known answer.\"\"\"\n",
    "            \n",
    "        self.__generate_test_training_sets(train_set_size)\n",
    "        \n",
    "        # training loop for SGD\n",
    "        for x in range(iterations):\n",
    "            test_item_pair = random.choice(self.training_set)\n",
    "            known_output = test_item_pair[1]\n",
    "            self.__feed_forward(test_item_pair[0])\n",
    "            self.__back_propagate_error(known_output, self.output)\n",
    "            delta_weight_matrix, delta_biases_matrix = self.__create_deltas()\n",
    "            \n",
    "            # adjust weights and biases by delta for the layer\n",
    "            self.weights = np.add(self.weights, delta_weight_matrix)\n",
    "            self.biases = np.add(self.biases, delta_biases_matrix)\n",
    "            \n",
    "\n",
    "\n",
    "    def __generate_test_training_sets(self, train_set_size):\n",
    "        \"\"\"This method will create the testing and training sets for the network\n",
    "            to test accuracy. These lists are built from the list of all input pairs.\n",
    "            This set of pairs will be constrained with a percentage\n",
    "            of the total set. This percentage is represented as train_set_size. So for example\n",
    "            if this method is set to preform with a training set size of .7 then \n",
    "            It will randomly select values from 70% of the list and reserve 30% to test on later. \n",
    "            \"\"\"\n",
    "        if train_set_size <= 0.0 or isinstance(train_set_size, int):\n",
    "            raise ValueError('Training set size must be a positive floating point number')\n",
    "        \n",
    "        # find index corresponding to percentage of desired training set\n",
    "        split_index = math.floor( len(self.input_pairs)* train_set_size)\n",
    "        \n",
    "        # split self.input_pairs along percentage\n",
    "        self.training_set = self.input_pairs[:split_index]\n",
    "        self.testing_set = self.input_pairs[split_index:]\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def guess(self, input_list):\n",
    "        \"\"\"Given a sample input list show what the network would output\"\"\"\n",
    "        return self.__feed_forward(input_list, True).transpose().flatten()\n",
    "\n",
    "    def test_network(self):\n",
    "        total_tests = len(self.testing_set)\n",
    "        total_correct = 0\n",
    "        vectorized_normalize = np.vectorize(lambda x: 1.0 if x >= .5 else 0.0)\n",
    "        for member in self.testing_set:\n",
    "            guess  = self.guess(member[0])\n",
    "            guess = vectorized_normalize(guess)\n",
    "            if np.array_equal(guess, member[1]):\n",
    "                total_correct += 1\n",
    "        \n",
    "        self.accuracy = total_correct / total_tests\n",
    "        \n",
    "    @staticmethod\n",
    "    def save_network(network, file_name):\n",
    "        joblib.dump(network,file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_network(file_name):\n",
    "        return joblib.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51566809]]\n",
      "[[0.5070556]]\n",
      "[[0.94374375]]\n",
      "[[0.06614843]]\n"
     ]
    }
   ],
   "source": [
    "xor_inputs = [[1, 1], [1, 0], [0, 1], [0, 0]]\n",
    "xor_answers = [[0], [1], [1], [0]]\n",
    "\n",
    "\n",
    "arch = [2,2,1]\n",
    "\n",
    "network = NeuralNetwork(arch,inputs=xor_inputs,outputs=xor_answers,learning_rate=.01)\n",
    "\n",
    "network.train_SGD(100000)\n",
    "\n",
    "for input in xor_inputs:\n",
    "    print(network.guess(input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "arch = [4,5,5,3]\n",
    "data = pd.DataFrame(data=np.c_[data['data'], data['target']], columns=data['feature_names'] + ['target'])\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "targets = data['target']\n",
    "\n",
    "data.drop(['target'], axis=1, inplace=True)\n",
    "data.head()\n",
    "\n",
    "data = data.to_numpy()\n",
    "data = [x.tolist() for x in data]\n",
    "\n",
    "def build_outputs(x):\n",
    "    if x == 0.0:\n",
    "        return np.array([1.0, 0.0, 0.0])\n",
    "    elif x == 1.0:\n",
    "        return np.array([0.0,1.0,0.0])\n",
    "    elif x == 2.0:\n",
    "        return np.array([0.0,0.0,1.0])\n",
    "    \n",
    "targets = [build_outputs(x) for x in targets]\n",
    "\n",
    "iris_network = NeuralNetwork(arch, data, targets, .01)\n",
    "\n",
    "iris_network.train_SGD(1000000,.8)\n",
    "\n",
    "iris_network.test_network()\n",
    "\n",
    "NeuralNetwork.save_network(iris_network,'network.pkl')\n",
    "\n",
    "print(\"Done Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "iris_network = NeuralNetwork.load_network('network.pkl')\n",
    "\n",
    "print(iris_network.accuracy)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
